{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input/vgg19\"))\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir('../input/dr-data-processed/'))\n",
    "print(os.listdir('../input/pretrained-weights-vgg19'))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Load 2015 data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_19_dir = '../input/dr-data-processed/'\n",
    "\n",
    "train_15_class_true = pd.read_csv(os.path.join(data_19_dir, 'labels/trainLabels15.csv'))\n",
    "train_15_class_true.rename(columns={'image':'id_code', 'level':'diagnosis'}, inplace=True)\n",
    "train_15_class_true[\"id_code\"]=train_15_class_true[\"id_code\"].apply(lambda x: \"train_15_processed/\" + x + \".jpg\")\n",
    "print(train_15_class_true.head())\n",
    "plt.figure(1)\n",
    "plt.hist(train_15_class_true['diagnosis'], bins=5)\n",
    "plt.title('train 15 distribution')\n",
    "\n",
    "test_15_class_true = pd.read_csv(os.path.join(data_19_dir, 'labels/testLabels15.csv'))\n",
    "test_15_class_true.rename(columns={'image':'id_code', 'level':'diagnosis'}, inplace=True)\n",
    "test_15_class_true[\"id_code\"]=test_15_class_true[\"id_code\"].apply(lambda x: \"test_15_processed/\" + x + \".jpg\")\n",
    "test_15_class_true = test_15_class_true.drop(columns=['Usage'])\n",
    "print(test_15_class_true.head())\n",
    "plt.figure(2)\n",
    "plt.hist(test_15_class_true['diagnosis'], bins=5)\n",
    "plt.title('test 15 distribution')\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "pretrain_class_true = pd.concat([train_15_class_true, test_15_class_true], axis=0, ignore_index=True)\n",
    "pretrain_class_true = shuffle(pretrain_class_true)\n",
    "pretrain_class_true = pretrain_class_true.reset_index(drop=True)\n",
    "class_count = pretrain_class_true['diagnosis'].value_counts()\n",
    "pretrain_class_weight = {c: np.sqrt(np.max(class_count)/class_count[c]) for c in [0, 1, 2, 3, 4]}\n",
    "print(pretrain_class_weight)\n",
    "pretrain_class_true['diagnosis'] = pretrain_class_true['diagnosis'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Preprocess images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "global C, L\n",
    "C = 5\n",
    "L = 224\n",
    "\n",
    "# Preprocessing functions\n",
    "\"\"\"\n",
    "remove the black borders of an img\n",
    "\"\"\"\n",
    "def cut_black(img, tol=5):\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    mask = img_gray > tol\n",
    "    idx = np.ix_(mask.any(1),mask.any(0))\n",
    "    return img[idx[0], idx[1], :]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Only take the center\n",
    "\"\"\"\n",
    "def crop_center(img):\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    if H == W:\n",
    "        return img\n",
    "    elif H > W:\n",
    "        return img[H//2-W//2:H//2+W//2, :, :]\n",
    "    else:\n",
    "        return img[:, W//2-H//2:W//2+H//2, :]\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "Adjust brightness, scale to mean 100\n",
    "\"\"\"\n",
    "def adjust_light(img):\n",
    "    brightness = np.mean(img)\n",
    "    img = np.clip(100.0/brightness*img, 0, 255).astype('uint8')\n",
    "    return img\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Crop image based off black pixels on diagon\n",
    "\"\"\"\n",
    "def crop_diagonal(img, tol=5):\n",
    "    img_diag = np.diagonal(img)\n",
    "    img_diag_gray = np.mean(img_diag, axis=0).astype('int32')\n",
    "    idx0 = np.argmax(img_diag_gray>tol)\n",
    "    idx1 = len(img_diag_gray) - np.argmax(img_diag_gray[::-1]>tol)\n",
    "    return img[idx0:idx1, idx0:idx1, :]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Subtract median blur\n",
    "\"\"\"\n",
    "def subtract_median_bg_image(img):\n",
    "    k = np.max(img.shape)//20*2+1\n",
    "    bg = cv2.medianBlur(img, k)\n",
    "    return cv2.addWeighted(img, 4, bg, -4, 128)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Full preprocessing steps\n",
    "\"\"\"\n",
    "def img_preprocess(img):\n",
    "    img = cut_black(img)\n",
    "    img = crop_center(img)\n",
    "    img = crop_diagonal(img)\n",
    "    #img = adjust_light(img)\n",
    "    img = subtract_median_bg_image(img)\n",
    "    img = cv2.resize(img, dsize=(L, L), interpolation=cv2.INTER_CUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../input/aptos2019-blindness-detection/\"\n",
    "train_dir = os.path.join(data_dir, \"train_images\")\n",
    "test_dir = os.path.join(data_dir, \"test_images\")\n",
    "train_dir_processed = \"../input/train_processed\"\n",
    "test_dir_processed = \"../input/test_processed\"\n",
    "train_label_file = os.path.join(data_dir, \"train.csv\")\n",
    "test_name_file = os.path.join(data_dir, \"test.csv\")\n",
    "\n",
    "if not os.path.exists(train_dir_processed):\n",
    "    os.mkdir(train_dir_processed)\n",
    "if not os.path.exists(test_dir_processed):\n",
    "    os.mkdir(test_dir_processed)\n",
    "\n",
    "train_labels = pd.read_csv(train_label_file)\n",
    "classes = train_labels['diagnosis'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = os.listdir(train_dir)\n",
    "N_train = len(train_imgs)\n",
    "print(\"process train, n = \" + str(N_train))\n",
    "os.system('echo ' + 'process train, n = ' + str(N_train))\n",
    "for train_img in train_imgs:\n",
    "    img = cv2.imread(os.path.join(train_dir, train_img))\n",
    "    img = img_preprocess(img)\n",
    "    cv2.imwrite(os.path.join(train_dir_processed, train_img), img)\n",
    "\n",
    "test_imgs = os.listdir(test_dir)\n",
    "N_test = len(test_imgs)\n",
    "print(\"process test, n = \" + str(N_test))\n",
    "os.system('echo ' + 'process test, n = ' + str(N_test))\n",
    "for test_img in test_imgs:\n",
    "    img = cv2.imread(os.path.join(test_dir, test_img))\n",
    "    img = img_preprocess(img)\n",
    "    cv2.imwrite(os.path.join(test_dir_processed, test_img), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Organize training & validation data for fit_generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_class_true = pd.read_csv(train_label_file)\n",
    "train_class_true[\"id_code\"]=train_class_true[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "train_class_true = shuffle(train_class_true)\n",
    "train_class_true = train_class_true.reset_index(drop=True)\n",
    "train_class_true['diagnosis'] = train_class_true['diagnosis'].astype(str)\n",
    "class_count = train_class_true['diagnosis'].value_counts()\n",
    "class_count = class_count.sort_index()\n",
    "class_weight = {c: np.sqrt(np.max(class_count)/class_count[c]) for c in [0, 1, 2, 3, 4]}\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Build model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\"\"\"\n",
    "Single conv layer in CNN model\n",
    "\"\"\"\n",
    "def conv_layer(x_in, filters, kernel_dim, drop_rate=0.0, batch_norm=True, max_pool=True):\n",
    "    x = tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_dim,kernel_dim), strides=(1,1), padding='same',\n",
    "               kernel_initializer='he_normal')(x_in)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    if drop_rate > 0.0:\n",
    "        x = tf.keras.layers.Dropout(drop_rate)(x)\n",
    "    if batch_norm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    if max_pool:\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Single dense/FC layer in CNN model\n",
    "\"\"\"\n",
    "def dense_layer(x_in, units, activation='tanh', drop_rate=0.0):\n",
    "    x = tf.keras.layers.Dense(units, activation=activation)(x_in)\n",
    "    if drop_rate > 0.0:\n",
    "        x = tf.keras.layers.Dropout(rate = drop_rate)(x)\n",
    "    return x\n",
    "\n",
    "\"\"\"\n",
    "Custom loss function\n",
    "\"\"\"\n",
    "def custom_loss(y_true, y_pred):\n",
    "    class_true = K.cast(K.expand_dims(K.argmax(y_true, axis=-1), axis=-1), 'float32')   # y_true is one-hot\n",
    "    i = np.array([[0, 1, 2, 3, 4]]).astype('float32')\n",
    "    alpha = 3 * 30 * K.square(i - class_true) / K.sum(K.square(i - class_true), axis=0)\n",
    "    # cross entropy\n",
    "    loss1 = -K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "    # additional term to penalize worse predictions\n",
    "    loss2 = -K.sum(alpha * (1-y_true) * K.log(1-y_pred), axis=-1)\n",
    "    \n",
    "    return loss1 + loss2\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Custom eval metric: quadratic weighted kappa\n",
    "\"\"\"\n",
    "def qwk(y_true, y_pred):\n",
    "    # compute confution matrix\n",
    "    y_true_label = K.argmax(y_true, axis=-1)    # one-hot to class number\n",
    "    y_pred_label = K.argmax(y_pred, axis=-1)\n",
    "    confusion = tf.math.confusion_matrix(y_true_label, y_pred_label, num_classes=5, dtype='float32')\n",
    "    \n",
    "    # compute quadratic weight\n",
    "    alpha = np.square([[i-j for i in range(5)] for j in range(5)]).astype('float32')\n",
    "\n",
    "    # compute observed and expected matrix\n",
    "    observed = confusion/tf.reduce_sum(confusion)  # count -> distribution\n",
    "    P_pred = tf.reduce_sum(confusion, axis=0)/tf.reduce_sum(confusion)\n",
    "    P_true = tf.reduce_sum(confusion, axis=1)/tf.reduce_sum(confusion)\n",
    "    expected = tf.tensordot(P_true, P_pred, axes=0)\n",
    "    \n",
    "    # compute kappa\n",
    "    kappa = 1 - tf.reduce_sum(tf.multiply(alpha, observed))/tf.reduce_sum(tf.multiply(alpha, expected))\n",
    "    return kappa\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Use transfer learning\n",
    "\"\"\"\n",
    "def transfer_model(loss, lr=1e-3):\n",
    "    #base_model = tf.keras.applications.VGG16(input_shape=(L,L,3),\n",
    "    #                                         weights='../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    #                                         include_top=False)\n",
    "    \n",
    "    base_model = tf.keras.applications.VGG19(input_shape=(L,L,3),\n",
    "                                             weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                             include_top=False)\n",
    "    \n",
    "    #base_model = tf.keras.applications.ResNet50(input_shape=(L,L,3),\n",
    "    #                                            weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    #                                            include_top=False)\n",
    "\n",
    "    #base_model = tf.keras.applications.ResNet50(input_shape=(L,L,3),\n",
    "    #                                            weights='../input/resnet50trainedwithaptosolddataset/Resnet50_bestqwk.h5',\n",
    "    #                                            include_top=True)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    x = base_model.output\n",
    "    \n",
    "    # dense layers\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)    # faster processing, omit spacial, vessel leakage can happen anywhere\n",
    "    #x = tf.keras.layers.GlobalMaxPooling2D()(x)         # you only need to find certain features once, other areas can be blank\n",
    "    #x = dense_layer(x, units=1024, activation='elu', drop_rate=0.4)\n",
    "    x = dense_layer(x, units=256, activation='tanh', drop_rate=0.4)\n",
    "    \n",
    "    # class pred\n",
    "    x_out = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "    \n",
    "    # compile model\n",
    "    model = tf.keras.models.Model(inputs=base_model.input, outputs=x_out)\n",
    "    #sgd = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "    adam = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    \n",
    "    if loss == 'categorical_crossentropy':\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc', qwk])\n",
    "    else:\n",
    "        model.compile(optimizer=adam, loss=custom_loss, metrics=['acc', qwk])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "Display training history\n",
    "\"\"\"\n",
    "def show_history(history):\n",
    "    acc=history.history['acc']\n",
    "    val_acc=history.history['val_acc']\n",
    "    qwk=history.history['qwk']\n",
    "    val_qwk=history.history['val_qwk']\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "    \n",
    "    epochs = list(range(1, len(acc)+1))\n",
    "    plt.figure(1)\n",
    "    plt.plot(epochs, loss, 'b', epochs, val_loss, 'r')\n",
    "    plt.title('loss')\n",
    "    plt.legend(('train', 'val'))\n",
    "    \n",
    "    plt.figure(2)\n",
    "    plt.plot(epochs, acc, 'b', epochs, val_acc, 'r')\n",
    "    plt.title('accuracy')\n",
    "    plt.legend(('train', 'val'))\n",
    "    \n",
    "    plt.figure(3)\n",
    "    plt.plot(epochs, qwk, 'b', epochs, val_qwk, 'r')\n",
    "    plt.title('weighted kappa')\n",
    "    plt.legend(('train', 'val'))\n",
    "\n",
    "    plt.figure(4)\n",
    "    plt.plot(loss, qwk)\n",
    "    plt.title('train: qwk vs loss')\n",
    "    \n",
    "    plt.figure(5)\n",
    "    plt.plot(val_loss, val_qwk)\n",
    "    plt.title('val: qwk vs loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Generate datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "print(\"Train set:\")\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                   #rotation_range=360,  # an eyeball at any angle is still the same\n",
    "                                   #horizontal_flip=True,        # left/right eye symmetry\n",
    "                                   #vertical_flip=True,          # left/right eye symmetry\n",
    "                                   samplewise_center=True,     # centering\n",
    "                                   samplewise_std_normalization=True, # standardizing\n",
    "                                   zca_whitening=True,\n",
    "                                   validation_split=0.1)\n",
    "    \n",
    "train_gen = train_datagen.flow_from_dataframe(dataframe=train_class_true,\n",
    "                                              directory = train_dir_processed,\n",
    "                                              batch_size=batch_size,\n",
    "                                              x_col = 'id_code',\n",
    "                                              y_col = 'diagnosis',\n",
    "                                              class_mode='categorical',\n",
    "                                              target_size=(L, L),\n",
    "                                              subset='training')\n",
    "    \n",
    "val_gen = train_datagen.flow_from_dataframe(dataframe=train_class_true,\n",
    "                                            directory = train_dir_processed,\n",
    "                                            batch_size=batch_size,\n",
    "                                            x_col = 'id_code',\n",
    "                                            y_col = 'diagnosis',\n",
    "                                            class_mode='categorical',\n",
    "                                            target_size=(L, L),\n",
    "                                            subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                      #rotation_range=360,  # an eyeball at any angle is still the same\n",
    "                                      #horizontal_flip=True,        # left/right eye symmetry\n",
    "                                      #vertical_flip=True,          # left/right eye symmetry\n",
    "                                      samplewise_center=True,     # centering\n",
    "                                      samplewise_std_normalization=True, # standardizing\n",
    "                                      zca_whitening=True)\n",
    "\n",
    "pretrain_gen = pretrain_datagen.flow_from_dataframe(dataframe=pretrain_class_true,\n",
    "                                                 directory = data_19_dir,\n",
    "                                                 batch_size=128,\n",
    "                                                 x_col = 'id_code',\n",
    "                                                 y_col = 'diagnosis',\n",
    "                                                 class_mode='categorical',\n",
    "                                                 target_size=(L, L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Pretrain model on 2015 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transfer_model(loss = 'categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "callbacks1 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n",
    "                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath = 'pretrained-weights-best-1.hdf5', \n",
    "                                                monitor='val_loss', save_best_only=True)]\n",
    "#             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 10))]\n",
    "\n",
    "callbacks2 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n",
    "                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath = 'pretrained-weights-best-2.hdf5', \n",
    "                                                monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "os.system('echo ' + 'pretraining')\n",
    "history = model.fit_generator(pretrain_gen,\n",
    "                              validation_data = val_gen,\n",
    "                              steps_per_epoch=math.ceil(pretrain_gen.samples/pretrain_gen.batch_size),\n",
    "                              class_weight=pretrain_class_weight,\n",
    "                              epochs = 15,\n",
    "                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n",
    "                              callbacks = callbacks1,\n",
    "                              use_multiprocessing=False,\n",
    "                              verbose = 1)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('echo ' + 'pretraining 2')\n",
    "model = transfer_model(loss = 'custom', lr=1e-4)\n",
    "model.load_weights('pretrained-weights-best-1.hdf5')\n",
    "#model.load_weights('../input/pretrained-weights-vgg16/pretrained-weights-best-1.hdf5')\n",
    "history = model.fit_generator(pretrain_gen,\n",
    "                              validation_data = val_gen,\n",
    "                              steps_per_epoch=math.ceil(pretrain_gen.samples/pretrain_gen.batch_size),\n",
    "                              class_weight=pretrain_class_weight,\n",
    "                              epochs = 15,\n",
    "                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n",
    "                              callbacks = callbacks2,\n",
    "                              use_multiprocessing=False,\n",
    "                              verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Fine tune on 2019 dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transfer_model(loss = 'categorical_crossentropy')\n",
    "#model.summary()\n",
    "#model.load_weights('../input/pretrained-weights-vgg19/pretrained-weights-best-1.hdf5')\n",
    "model.load_weights('pretrained-weights-best-2.hdf5')\n",
    "callbacks1 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n",
    "                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath = 'weights-best-1.hdf5', \n",
    "                                                monitor='val_loss', save_best_only=True)]\n",
    "#             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 10))]\n",
    "\n",
    "callbacks2 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n",
    "                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath = 'weights-best-2.hdf5', \n",
    "                                                monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "os.system('echo ' + 'training')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              validation_data = val_gen,\n",
    "                              steps_per_epoch=math.ceil(train_gen.samples/train_gen.batch_size),\n",
    "                              class_weight=class_weight,\n",
    "                              epochs = 30,\n",
    "                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n",
    "                              callbacks = callbacks1,\n",
    "                              use_multiprocessing=False,\n",
    "                              verbose = 1)\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('echo ' + 'training 2')\n",
    "model = transfer_model(loss = 'custom', lr=1e-4)\n",
    "model.load_weights('weights-best-1.hdf5')\n",
    "#model.load_weights('pretrained-weights-best-2.hdf5')\n",
    "history = model.fit_generator(train_gen,\n",
    "                              validation_data = val_gen,\n",
    "                              steps_per_epoch=math.ceil(train_gen.samples/train_gen.batch_size),\n",
    "                              class_weight=class_weight,\n",
    "                              epochs = 40,\n",
    "                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n",
    "                              callbacks = callbacks2,\n",
    "                              use_multiprocessing=False,\n",
    "                              verbose = 1)\n",
    "\n",
    "show_history(history)\n",
    "\n",
    "model.load_weights('weights-best-2.hdf5')\n",
    "val_pred = model.predict_generator(val_gen, workers=1)    # workers!=1 will mess up the order\n",
    "val_pred_class = np.argmax(val_pred, axis=1)\n",
    "plt.figure(6)\n",
    "plt.hist(val_pred_class, bins=5)\n",
    "plt.title('val pred distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Generate test set results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'submission.csv'\n",
    "\n",
    "# copied from a kernel\n",
    "test_class_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\n",
    "test_class_df[\"id_code\"] = test_class_df[\"id_code\"].apply(lambda x: x + \".png\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
    "                                  samplewise_center=True,\n",
    "                                  samplewise_std_normalization=True,\n",
    "                                  zca_whitening=True)\n",
    "test_gen = test_datagen.flow_from_dataframe(  \n",
    "        dataframe=test_class_df,\n",
    "        directory = test_dir_processed,    \n",
    "        x_col=\"id_code\",\n",
    "        target_size = (L, L),\n",
    "        #preprocessing_function=img_preprocess,\n",
    "        batch_size = 1,\n",
    "        shuffle = False,\n",
    "        class_mode = None\n",
    "        )\n",
    "\n",
    "test_gen.reset()\n",
    "\n",
    "predict = model.predict_generator(test_gen,\n",
    "                                  steps = len(test_gen.filenames),\n",
    "                                  use_multiprocessing=False)\n",
    "\n",
    "filenames = test_gen.filenames\n",
    "results = pd.DataFrame({\"id_code\":filenames,\n",
    "                        \"diagnosis\":np.argmax(predict,axis=1)})\n",
    "results['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])\n",
    "results.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_class = pd.read_csv(output_filename)\n",
    "#print(test_pred_class.head())\n",
    "plt.hist(test_pred_class['diagnosis'], bins=5)\n",
    "plt.title('test pred distribution')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
