{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/vgg19\"))\nprint(os.listdir(\"../input/resnet50\"))\nprint(os.listdir(\"../input/resnet50trainedwithaptosolddataset\"))\nprint(os.listdir(\"../input\"))\nprint(os.listdir('../input/dr-data-processed/'))\nprint(os.listdir('../input/pretrained-weights-vgg19'))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**0. Combine data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndata_19_dir = '../input/dr-data-processed/'\n\ntrain_15_class_true = pd.read_csv(os.path.join(data_19_dir, 'labels/trainLabels15.csv'))\ntrain_15_class_true.rename(columns={'image':'id_code', 'level':'diagnosis'}, inplace=True)\ntrain_15_class_true[\"id_code\"]=train_15_class_true[\"id_code\"].apply(lambda x: \"train_15_processed/\" + x + \".jpg\")\nprint(train_15_class_true.head())\nplt.figure(1)\nplt.hist(train_15_class_true['diagnosis'], bins=5)\nplt.title('train 15 distribution')\n\ntest_15_class_true = pd.read_csv(os.path.join(data_19_dir, 'labels/testLabels15.csv'))\ntest_15_class_true.rename(columns={'image':'id_code', 'level':'diagnosis'}, inplace=True)\ntest_15_class_true[\"id_code\"]=test_15_class_true[\"id_code\"].apply(lambda x: \"test_15_processed/\" + x + \".jpg\")\ntest_15_class_true = test_15_class_true.drop(columns=['Usage'])\nprint(test_15_class_true.head())\nplt.figure(2)\nplt.hist(test_15_class_true['diagnosis'], bins=5)\nplt.title('test 15 distribution')\n\nfrom sklearn.utils import shuffle\n\npretrain_class_true = pd.concat([train_15_class_true, test_15_class_true], axis=0, ignore_index=True)\npretrain_class_true = shuffle(pretrain_class_true)\npretrain_class_true = pretrain_class_true.reset_index(drop=True)\nclass_count = pretrain_class_true['diagnosis'].value_counts()\npretrain_class_weight = {c: np.sqrt(np.max(class_count)/class_count[c]) for c in [0, 1, 2, 3, 4]}\nprint(pretrain_class_weight)\npretrain_class_true['diagnosis'] = pretrain_class_true['diagnosis'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1. Preprocess images**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import random\nimport math\nimport cv2\n\nglobal C, L\nC = 5\nL = 224\n\n# Preprocessing functions\n\"\"\"\nremove the black borders of an img\n\"\"\"\ndef cut_black(img, tol=5):\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    mask = img_gray > tol\n    idx = np.ix_(mask.any(1),mask.any(0))\n    return img[idx[0], idx[1], :]\n\n\n\"\"\"\nOnly take the center\n\"\"\"\ndef crop_center(img):\n    H, W = img.shape[0], img.shape[1]\n    if H == W:\n        return img\n    elif H > W:\n        return img[H//2-W//2:H//2+W//2, :, :]\n    else:\n        return img[:, W//2-H//2:W//2+H//2, :]\n        \n        \n\"\"\"\nAdjust brightness, scale to mean 100\n\"\"\"\ndef adjust_light(img):\n    brightness = np.mean(img)\n    img = np.clip(100.0/brightness*img, 0, 255).astype('uint8')\n    return img\n\n\n\"\"\"\nCrop image based off black pixels on diagon\n\"\"\"\ndef crop_diagonal(img, tol=5):\n    img_diag = np.diagonal(img)\n    img_diag_gray = np.mean(img_diag, axis=0).astype('int32')\n    idx0 = np.argmax(img_diag_gray>tol)\n    idx1 = len(img_diag_gray) - np.argmax(img_diag_gray[::-1]>tol)\n    return img[idx0:idx1, idx0:idx1, :]\n\n\n\"\"\"\nSubtract median blur\n\"\"\"\ndef subtract_median_bg_image(img):\n    k = np.max(img.shape)//20*2+1\n    bg = cv2.medianBlur(img, k)\n    return cv2.addWeighted(img, 4, bg, -4, 128)\n\n\n\"\"\"\nFull preprocessing steps\n\"\"\"\ndef img_preprocess(img):\n    img = cut_black(img)\n    img = crop_center(img)\n    img = crop_diagonal(img)\n    #img = adjust_light(img)\n    img = subtract_median_bg_image(img)\n    img = cv2.resize(img, dsize=(L, L), interpolation=cv2.INTER_CUBIC)\n    return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_dir = \"../input/aptos2019-blindness-detection/\"\ntrain_dir = os.path.join(data_dir, \"train_images\")\ntest_dir = os.path.join(data_dir, \"test_images\")\ntrain_dir_processed = \"../input/train_processed\"\ntest_dir_processed = \"../input/test_processed\"\ntrain_label_file = os.path.join(data_dir, \"train.csv\")\ntest_name_file = os.path.join(data_dir, \"test.csv\")\n\nif not os.path.exists(train_dir_processed):\n    os.mkdir(train_dir_processed)\nif not os.path.exists(test_dir_processed):\n    os.mkdir(test_dir_processed)\n\ntrain_labels = pd.read_csv(train_label_file)\nclasses = train_labels['diagnosis'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_imgs = os.listdir(train_dir)\nN_train = len(train_imgs)\nprint(\"process train, n = \" + str(N_train))\nos.system('echo ' + 'process train, n = ' + str(N_train))\nfor train_img in train_imgs:\n    img = cv2.imread(os.path.join(train_dir, train_img))\n    img = img_preprocess(img)\n    cv2.imwrite(os.path.join(train_dir_processed, train_img), img)\n\ntest_imgs = os.listdir(test_dir)\nN_test = len(test_imgs)\nprint(\"process test, n = \" + str(N_test))\nos.system('echo ' + 'process test, n = ' + str(N_test))\nfor test_img in test_imgs:\n    img = cv2.imread(os.path.join(test_dir, test_img))\n    img = img_preprocess(img)\n    cv2.imwrite(os.path.join(test_dir_processed, test_img), img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Organize training & validation data for fit_generator**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\n\ntrain_class_true = pd.read_csv(train_label_file)\ntrain_class_true[\"id_code\"]=train_class_true[\"id_code\"].apply(lambda x: x + \".png\")\ntrain_class_true = shuffle(train_class_true)\ntrain_class_true = train_class_true.reset_index(drop=True)\ntrain_class_true['diagnosis'] = train_class_true['diagnosis'].astype(str)\nclass_count = train_class_true['diagnosis'].value_counts()\nclass_count = class_count.sort_index()\nclass_weight = {c: np.sqrt(np.max(class_count)/class_count[c]) for c in [0, 1, 2, 3, 4]}\nprint(class_weight)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Build model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow.keras.backend as K\n\n\"\"\"\nSingle conv layer in CNN model\n\"\"\"\ndef conv_layer(x_in, filters, kernel_dim, drop_rate=0.0, batch_norm=True, max_pool=True):\n    x = tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_dim,kernel_dim), strides=(1,1), padding='same',\n               kernel_initializer='he_normal')(x_in)\n    x = tf.keras.layers.Activation('relu')(x)\n    if drop_rate > 0.0:\n        x = tf.keras.layers.Dropout(drop_rate)(x)\n    if batch_norm:\n        x = tf.keras.layers.BatchNormalization()(x)\n    if max_pool:\n        x = tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2))(x)\n    return x\n    \n\n\"\"\"\nSingle dense/FC layer in CNN model\n\"\"\"\ndef dense_layer(x_in, units, activation='tanh', drop_rate=0.0):\n    x = tf.keras.layers.Dense(units, activation=activation)(x_in)\n    if drop_rate > 0.0:\n        x = tf.keras.layers.Dropout(rate = drop_rate)(x)\n    return x\n\n\"\"\"\nCustom loss function\n\"\"\"\ndef custom_loss(y_true, y_pred):\n    class_true = K.cast(K.expand_dims(K.argmax(y_true, axis=-1), axis=-1), 'float32')   # y_true is one-hot\n    i = np.array([[0, 1, 2, 3, 4]]).astype('float32')\n    alpha = 3 * 30 * K.square(i - class_true) / K.sum(K.square(i - class_true), axis=0)\n    # cross entropy\n    loss1 = -K.sum(y_true * K.log(y_pred), axis=-1)\n    # additional term to penalize worse predictions\n    loss2 = -K.sum(alpha * (1-y_true) * K.log(1-y_pred), axis=-1)\n    \n    return loss1 + loss2\n\n\n\"\"\"\nCustom eval metric: quadratic weighted kappa\n\"\"\"\ndef qwk(y_true, y_pred):\n    # compute confution matrix\n    y_true_label = K.argmax(y_true, axis=-1)    # one-hot to class number\n    y_pred_label = K.argmax(y_pred, axis=-1)\n    confusion = tf.math.confusion_matrix(y_true_label, y_pred_label, num_classes=5, dtype='float32')\n    \n    # compute quadratic weight\n    alpha = np.square([[i-j for i in range(5)] for j in range(5)]).astype('float32')\n\n    # compute observed and expected matrix\n    observed = confusion/tf.reduce_sum(confusion)  # count -> distribution\n    P_pred = tf.reduce_sum(confusion, axis=0)/tf.reduce_sum(confusion)\n    P_true = tf.reduce_sum(confusion, axis=1)/tf.reduce_sum(confusion)\n    expected = tf.tensordot(P_true, P_pred, axes=0)\n    \n    # compute kappa\n    kappa = 1 - tf.reduce_sum(tf.multiply(alpha, observed))/tf.reduce_sum(tf.multiply(alpha, expected))\n    return kappa\n\n\n\"\"\"\nUse transfer learning\n\"\"\"\ndef transfer_model(loss, lr=1e-3):\n    #base_model = tf.keras.applications.VGG16(input_shape=(L,L,3),\n    #                                         weights='../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n    #                                         include_top=False)\n    \n    base_model = tf.keras.applications.VGG19(input_shape=(L,L,3),\n                                             weights = '../input/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n                                             include_top=False)\n    \n    #base_model = tf.keras.applications.ResNet50(input_shape=(L,L,3),\n    #                                            weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n    #                                            include_top=False)\n\n    #base_model = tf.keras.applications.ResNet50(input_shape=(L,L,3),\n    #                                            weights='../input/resnet50trainedwithaptosolddataset/Resnet50_bestqwk.h5',\n    #                                            include_top=True)\n    \n    for layer in base_model.layers:\n        layer.trainable = False\n    x = base_model.output\n    \n    # dense layers\n    #x = tf.keras.layers.Flatten()(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)    # faster processing, omit spacial, vessel leakage can happen anywhere\n    #x = tf.keras.layers.GlobalMaxPooling2D()(x)         # you only need to find certain features once, other areas can be blank\n    #x = dense_layer(x, units=1024, activation='elu', drop_rate=0.4)\n    x = dense_layer(x, units=256, activation='tanh', drop_rate=0.4)\n    \n    # class pred\n    x_out = tf.keras.layers.Dense(5, activation='softmax')(x)\n    \n    # compile model\n    model = tf.keras.models.Model(inputs=base_model.input, outputs=x_out)\n    #sgd = tf.keras.optimizers.SGD(learning_rate=1e-1)\n    adam = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n    \n    if loss == 'categorical_crossentropy':\n        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['acc', qwk])\n    else:\n        model.compile(optimizer=adam, loss=custom_loss, metrics=['acc', qwk])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\"\"\"\nDisplay training history\n\"\"\"\ndef show_history(history):\n    acc=history.history['acc']\n    val_acc=history.history['val_acc']\n    qwk=history.history['qwk']\n    val_qwk=history.history['val_qwk']\n    loss=history.history['loss']\n    val_loss=history.history['val_loss']\n    \n    epochs = list(range(1, len(acc)+1))\n    plt.figure(1)\n    plt.plot(epochs, loss, 'b', epochs, val_loss, 'r')\n    plt.title('loss')\n    plt.legend(('train', 'val'))\n    \n    plt.figure(2)\n    plt.plot(epochs, acc, 'b', epochs, val_acc, 'r')\n    plt.title('accuracy')\n    plt.legend(('train', 'val'))\n    \n    plt.figure(3)\n    plt.plot(epochs, qwk, 'b', epochs, val_qwk, 'r')\n    plt.title('weighted kappa')\n    plt.legend(('train', 'val'))\n\n    plt.figure(4)\n    plt.plot(loss, qwk)\n    plt.title('train: qwk vs loss')\n    \n    plt.figure(5)\n    plt.plot(val_loss, val_qwk)\n    plt.title('val: qwk vs loss')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4. Generate datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 128\n\nprint(\"Train set:\")\ntrain_datagen = ImageDataGenerator(rescale=1.0/255.0,\n                                   #rotation_range=360,  # an eyeball at any angle is still the same\n                                   #horizontal_flip=True,        # left/right eye symmetry\n                                   #vertical_flip=True,          # left/right eye symmetry\n                                   samplewise_center=True,     # centering\n                                   samplewise_std_normalization=True, # standardizing\n                                   zca_whitening=True,\n                                   validation_split=0.1)\n    \ntrain_gen = train_datagen.flow_from_dataframe(dataframe=train_class_true,\n                                              directory = train_dir_processed,\n                                              batch_size=batch_size,\n                                              x_col = 'id_code',\n                                              y_col = 'diagnosis',\n                                              class_mode='categorical',\n                                              target_size=(L, L),\n                                              subset='training')\n    \nval_gen = train_datagen.flow_from_dataframe(dataframe=train_class_true,\n                                            directory = train_dir_processed,\n                                            batch_size=batch_size,\n                                            x_col = 'id_code',\n                                            y_col = 'diagnosis',\n                                            class_mode='categorical',\n                                            target_size=(L, L),\n                                            subset='validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrain_datagen = ImageDataGenerator(rescale=1.0/255.0,\n                                      #rotation_range=360,  # an eyeball at any angle is still the same\n                                      #horizontal_flip=True,        # left/right eye symmetry\n                                      #vertical_flip=True,          # left/right eye symmetry\n                                      samplewise_center=True,     # centering\n                                      samplewise_std_normalization=True, # standardizing\n                                      zca_whitening=True)\n\npretrain_gen = pretrain_datagen.flow_from_dataframe(dataframe=pretrain_class_true,\n                                                 directory = data_19_dir,\n                                                 batch_size=128,\n                                                 x_col = 'id_code',\n                                                 y_col = 'diagnosis',\n                                                 class_mode='categorical',\n                                                 target_size=(L, L))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Pretrain model on 2015 dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = transfer_model(loss = 'categorical_crossentropy')\nmodel.summary()\n\ncallbacks1 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n             tf.keras.callbacks.ModelCheckpoint(filepath = 'pretrained-weights-best-1.hdf5', \n                                                monitor='val_loss', save_best_only=True)]\n#             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 10))]\n\ncallbacks2 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n             tf.keras.callbacks.ModelCheckpoint(filepath = 'pretrained-weights-best-2.hdf5', \n                                                monitor='val_loss', save_best_only=True)]\n\n#os.system('echo ' + 'pretraining')\n#history = model.fit_generator(pretrain_gen,\n#                              validation_data = val_gen,\n#                              steps_per_epoch=math.ceil(pretrain_gen.samples/pretrain_gen.batch_size),\n#                              class_weight=pretrain_class_weight,\n#                              epochs = 15,\n#                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n#                              callbacks = callbacks1,\n#                              use_multiprocessing=False,\n#                              verbose = 1)\n#\n#show_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('echo ' + 'pretraining 2')\n#model = transfer_model(loss = 'custom', lr=1e-4)\n#model.load_weights('pretrained-weights-best-1.hdf5')\n#model.load_weights('../input/pretrained-weights-vgg16/pretrained-weights-best-1.hdf5')\n#history = model.fit_generator(pretrain_gen,\n#                              validation_data = val_gen,\n#                              steps_per_epoch=math.ceil(pretrain_gen.samples/pretrain_gen.batch_size),\n#                              class_weight=pretrain_class_weight,\n#                              epochs = 15,\n#                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n#                              callbacks = callbacks2,\n#                              use_multiprocessing=False,\n#                              verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**6. Fine tune on 2019 dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = transfer_model(loss = 'categorical_crossentropy')\n#model.summary()\nmodel.load_weights('../input/pretrained-weights-vgg19/pretrained-weights-best-1.hdf5')\n#model.load_weights('pretrained-weights-best-2.hdf5')\ncallbacks1 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n             tf.keras.callbacks.ModelCheckpoint(filepath = 'weights-best-1.hdf5', \n                                                monitor='val_loss', save_best_only=True)]\n#             tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 10))]\n\ncallbacks2 = [#tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7),\n             tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',  min_delta=0.0004,\n                                                  patience=2, factor=0.5, min_lr=1e-5,  mode='auto', verbose=1),\n             tf.keras.callbacks.ModelCheckpoint(filepath = 'weights-best-2.hdf5', \n                                                monitor='val_loss', save_best_only=True)]\n\nos.system('echo ' + 'training')\nhistory = model.fit_generator(train_gen,\n                              validation_data = val_gen,\n                              steps_per_epoch=math.ceil(train_gen.samples/train_gen.batch_size),\n                              class_weight=class_weight,\n                              epochs = 30,\n                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n                              callbacks = callbacks1,\n                              use_multiprocessing=False,\n                              verbose = 1)\n\nshow_history(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('echo ' + 'training 2')\nmodel = transfer_model(loss = 'custom', lr=1e-4)\nmodel.load_weights('weights-best-1.hdf5')\n#model.load_weights('pretrained-weights-best-2.hdf5')\nhistory = model.fit_generator(train_gen,\n                              validation_data = val_gen,\n                              steps_per_epoch=math.ceil(train_gen.samples/train_gen.batch_size),\n                              class_weight=class_weight,\n                              epochs = 40,\n                              validation_steps=math.ceil(val_gen.samples/val_gen.batch_size),\n                              callbacks = callbacks2,\n                              use_multiprocessing=False,\n                              verbose = 1)\n\nshow_history(history)\n\nmodel.load_weights('weights-best-2.hdf5')\nval_pred = model.predict_generator(val_gen, workers=1)    # workers!=1 will mess up the order\nval_pred_class = np.argmax(val_pred, axis=1)\nplt.figure(6)\nplt.hist(val_pred_class, bins=5)\nplt.title('val pred distribution')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**5. Generate test set results**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"test_datagen = ImageDataGenerator(rescale=1.0/255.0,\n                                  samplewise_center=True,\n                                  samplewise_std_normalization=True,\n                                  zca_whitening=True)\ntest_gen = test_datagen.flow_from_dataframe(  \n        dataframe=test_15_class_true,\n        directory = test,    \n        x_col=\"id_code\",\n        target_size = (L, L),\n        #preprocessing_function=img_preprocess,\n        batch_size = 1,\n        shuffle = False,\n        class_mode = None\n        )\n\ntest_pred = model.predict_generator(test_gen, workers=1)    # workers!=1 will mess up the order\ntest_pred_class = np.argmax(test_pred, axis=1)\nplt.figure(7)\nplt.hist(test_pred_class, bins=5)\nplt.title('test pred distribution')"},{"metadata":{"trusted":true},"cell_type":"code","source":"output_filename = 'submission.csv'\n\n# copied from a kernel\ntest_class_df = pd.read_csv('../input/aptos2019-blindness-detection/sample_submission.csv')\ntest_class_df[\"id_code\"] = test_class_df[\"id_code\"].apply(lambda x: x + \".png\")\n\ntest_datagen = ImageDataGenerator(rescale=1.0/255.0,\n                                  samplewise_center=True,\n                                  samplewise_std_normalization=True,\n                                  zca_whitening=True)\ntest_gen = test_datagen.flow_from_dataframe(  \n        dataframe=test_class_df,\n        directory = test_dir_processed,    \n        x_col=\"id_code\",\n        target_size = (L, L),\n        #preprocessing_function=img_preprocess,\n        batch_size = 1,\n        shuffle = False,\n        class_mode = None\n        )\n\ntest_gen.reset()\n\npredict = model.predict_generator(test_gen,\n                                  steps = len(test_gen.filenames),\n                                  use_multiprocessing=False)\n\nfilenames = test_gen.filenames\nresults = pd.DataFrame({\"id_code\":filenames,\n                        \"diagnosis\":np.argmax(predict,axis=1)})\nresults['id_code'] = results['id_code'].map(lambda x: str(x)[:-4])\nresults.to_csv(output_filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred_class = pd.read_csv(output_filename)\n#print(test_pred_class.head())\nplt.hist(test_pred_class['diagnosis'], bins=5)\nplt.title('test pred distribution')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}